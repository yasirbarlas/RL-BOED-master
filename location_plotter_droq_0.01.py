import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import torch

# Seeds are 10, 50, 100 for evaluation time in select_policy_env.py

# Example with K = 2 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432]]

# actions = [
#     [-0.0685, 0.0535],
#     [0.1438, 0.1110],
#     [0.0071, -0.1892],
#     [0.1159, -0.1618],
#     [-0.0641, -0.1945],
#     [-0.1121, -0.1731],
#     [-0.1353, -0.1562],
#     [-0.1255, -0.1227],
#     [-0.2310, 0.1168],
#     [0.0922, 0.4867],
#     [-0.0009, 0.3228],
#     [-0.0175, 0.2894],
#     [0.1421, 0.2914],
#     [0.0105, 0.3426],
#     [-0.0308, 0.3600],
#     [-0.1020, -0.0481],
#     [-0.0296, 0.3230],
#     [-0.0640, 0.2268],
#     [-0.0586, -0.1031],
#     [0.0195, 0.2644],
#     [-0.0331, -0.1393],
#     [0.0177, 0.3997],
#     [0.1398, 0.3296],
#     [0.0651, 0.3581],
#     [-0.0567, 0.1989],
#     [0.0507, -0.0537],
#     [-0.0081, -0.0360],
#     [0.0313, 0.1902],
#     [-0.0188, 0.0028],
#     [0.0352, 0.2335]
# ]

# Reward in sPCE
#torch.tensor([[ 0.3598,  0.9986,  2.4417,  3.5967,  4.5815,  5.5261,  5.9300,  6.6609,
#          6.8330,  8.8042,  9.6506, 10.6454, 10.8593, 10.2326, 10.4725,  9.9973,
#         10.0598, 10.0510, 10.4126,  9.0457,  9.8482, 10.4821, 12.9321, 13.0663,
#         12.9943, 12.9033, 13.0541, 13.0480, 13.0956, 13.0850]],
#       device='cuda:0')

####################################################

# Example with K = 2 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784]]

# actions = [
#     [-0.0232, 0.0282],
#     [0.1153, -0.2689],
#     [0.2073, -0.0711],
#     [-0.0547, -0.2144],
#     [-0.2715, 0.0855],
#     [0.1167, 0.2795],
#     [-0.1055, -0.3306],
#     [-0.1693, -0.3043],
#     [-0.1204, -0.3077],
#     [-0.0915, -0.3224],
#     [-0.3712, -0.1971],
#     [-0.1882, -0.3718],
#     [0.0072, -0.4342],
#     [-0.4463, -0.2314],
#     [-0.4200, -0.1656],
#     [-0.1915, -0.3135],
#     [-0.5656, 0.2033],
#     [-0.2207, -0.4360],
#     [-0.2323, -0.4197],
#     [-0.2892, -0.4773],
#     [0.1262, -0.3256],
#     [0.6464, -0.1629],
#     [-0.4187, -0.1598],
#     [0.0439, -0.5705],
#     [-0.0007, -0.5729],
#     [-0.4769, -0.0023],
#     [-0.4968, -0.0754],
#     [0.1912, -0.1552],
#     [-0.3383, -0.3917],
#     [-0.2905, -0.5765]
# ]

# Reward in sPCE
#torch.tensor([[0.2121, 0.4736, 1.1129, 2.2731, 2.8930, 4.0541, 5.0850, 6.0397, 6.2205,
#         6.4508, 6.5666, 6.6376, 7.2863, 7.2461, 7.2550, 8.6103, 8.6399, 8.4818,
#         8.4135, 8.2594, 8.7969, 8.9832, 8.9853, 9.0718, 9.0303, 8.9992, 8.9861,
#         9.8591, 9.8784, 9.7615]], device='cuda:0')

####################################################

# Example with K = 2 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402]]

# actions = [
#     [0.0433, -0.0125],
#     [-0.0346, 0.0247],
#     [-0.0477, 0.0515],
#     [0.3092, 0.0116],
#     [-0.1842, -0.2379],
#     [-0.1644, 0.4139],
#     [-0.0683, 0.4203],
#     [-0.3434, 0.2678],
#     [-0.3605, 0.2529],
#     [0.0212, 0.4743],
#     [0.0517, 0.2998],
#     [-0.2743, 0.2586],
#     [-0.0348, 0.1669],
#     [-0.0696, 0.1763],
#     [-0.3362, 0.1955],
#     [-0.1873, 0.1181],
#     [-0.0982, 0.1464],
#     [-0.1847, 0.0831],
#     [-0.0508, -0.0270],
#     [-0.2662, 0.0838],
#     [-0.1644, 0.0394],
#     [-0.2301, 0.1025],
#     [-0.2976, 0.1866],
#     [-0.1176, 0.1949],
#     [-0.0465, -0.1650],
#     [-0.0670, 0.2988],
#     [-0.2217, 0.0102],
#     [0.0672, 0.1310],
#     [-0.0293, 0.1408],
#     [-0.1035, 0.0429]
# ]

# Reward in sPCE
#torch.tensor([[ 0.8457,  3.0624,  5.3369,  6.4029,  7.5625,  8.4447,  8.8962,  9.8356,
#         10.1782, 10.2517, 10.4633, 10.6473, 10.7785, 10.8348, 11.0394, 11.0391,
#         11.3155, 11.5851, 11.6619, 11.7748, 11.9489, 11.9162, 11.8951, 11.8554,
#         11.8576, 11.8290, 11.8824, 11.9249, 11.7565, 12.0538]],
#       device='cuda:0')

####################################################

# Example with K = 1 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982]]

# actions = [
#     [-0.0272, 0.0641],
#     [0.2384, -0.2970],
#     [0.1233, -0.3150],
#     [0.3022, -0.0948],
#     [-0.0548, -0.3451],
#     [0.3286, 0.4197],
#     [-0.4081, 0.1301],
#     [0.2698, 0.1320],
#     [-0.4868, -0.2544],
#     [-0.2332, 0.6403],
#     [0.0303, -0.6685],
#     [0.6304, -0.2826],
#     [0.5998, -0.3031],
#     [0.5860, -0.1316],
#     [0.5044, -0.5815],
#     [0.6010, -0.1752],
#     [0.6236, -0.1984],
#     [0.3863, -0.6888],
#     [0.5832, -0.3260],
#     [0.5697, -0.2361],
#     [0.5066, -0.5273],
#     [0.4952, -0.1938],
#     [0.2698, -0.5268],
#     [0.5105, 0.0633],
#     [0.3096, -0.5944],
#     [0.2976, -0.4370],
#     [0.3725, -0.5081],
#     [0.4461, -0.5496],
#     [0.1990, -0.6505],
#     [0.4756, -0.6501]
# ]

# Reward in sPCE
#torch.tensor([[1.4099, 3.7714, 4.4476, 5.1188, 5.2743, 5.3318, 5.3139, 5.3259, 5.3443,
#         5.3558, 5.3868, 5.7292, 5.7124, 5.8159, 5.6844, 4.5981, 4.9821, 4.9935,
#         4.4902, 4.5395, 4.4973, 5.0420, 4.3926, 4.3817, 4.6677, 6.6406, 6.9359,
#         7.0910, 7.1896, 7.1248]], device='cuda:0')

####################################################

# Example with K = 1 and DroQ-0.01
# thetas = [[-1.4033,  1.8134]]

# actions = [
#     [0.0860, 0.1226],
#     [-0.2190, -0.2049],
#     [0.2113, -0.3297],
#     [-0.3546, 0.2270],
#     [-0.0157, 0.4755],
#     [-0.4327, 0.0689],
#     [0.5247, 0.0753],
#     [-0.5374, -0.4274],
#     [-0.2926, 0.4936],
#     [-0.2088, 0.5554],
#     [-0.1937, 0.5513],
#     [-0.2930, 0.4609],
#     [-0.3423, 0.4884],
#     [-0.2196, 0.3379],
#     [0.1532, -0.6785],
#     [-0.2682, 0.6206],
#     [-0.2061, 0.5102],
#     [-0.2503, 0.5691],
#     [-0.1738, 0.6040],
#     [-0.4764, 0.3167],
#     [-0.2670, 0.3419],
#     [-0.1502, 0.6084],
#     [-0.4551, 0.5075],
#     [-0.0525, 0.3315],
#     [-0.0729, 0.4681],
#     [-0.0829, 0.5214],
#     [-0.1753, 0.5245],
#     [-0.4912, 0.5242],
#     [-0.2805, 0.3708],
#     [-0.3932, 0.3567]
# ]

# Reward in sPCE
#torch.tensor([[1.0692, 1.3655, 1.9590, 3.2371, 4.3664, 4.8018, 4.9134, 4.8337, 6.2669,
#         6.5585, 6.6596, 6.9199, 8.4366, 7.9973, 8.0233, 7.7716, 7.9234, 8.2097,
#         8.2222, 8.2968, 8.1261, 8.1872, 8.7272, 8.8239, 8.8488, 8.8047, 8.7647,
#         8.8284, 8.9305, 8.4166]], device='cuda:0')

####################################################

# Example with K = 1 and DroQ-0.01
# thetas = [[-0.1029,  1.6810]]

# actions = [
#     [0.0514, 0.0313],
#     [-0.1991, -0.2441],
#     [0.3100, -0.0245],
#     [-0.3035, 0.3949],
#     [-0.0522, 0.5828],
#     [0.2538, 0.4234],
#     [-0.4869, 0.1666],
#     [0.5503, -0.3059],
#     [0.1373, -0.5547],
#     [0.3697, 0.7281],
#     [0.4548, 0.5290],
#     [-0.3414, 0.5720],
#     [-0.2200, 0.4903],
#     [-0.2533, 0.4350],
#     [-0.5466, 0.1455],
#     [0.5934, -0.3323],
#     [0.5470, -0.6747],
#     [-0.3690, -0.6952],
#     [-0.1696, 0.7791],
#     [0.1555, 0.8121],
#     [-0.3041, 0.5386],
#     [-0.4454, 0.5276],
#     [-0.2639, 0.4361],
#     [-0.5547, 0.0171],
#     [0.8371, 0.0957],
#     [-0.0460, 0.4219],
#     [-0.0731, 0.2548],
#     [0.1756, 0.2054],
#     [0.2205, 0.0497],
#     [-0.3923, -0.5355]
# ]

# Reward in sPCE
#torch.tensor([[0.7772, 1.4299, 1.5556, 1.8136, 2.0943, 2.6695, 2.7778, 3.0821, 3.0171,
#         2.5678, 2.4634, 2.5730, 3.0957, 3.4032, 3.4315, 3.5559, 3.5761, 3.5893,
#         4.3385, 3.8183, 3.8918, 3.8742, 3.8626, 3.7888, 3.7499, 4.7081, 4.6207,
#         4.3842, 4.4103, 4.4204]], device='cuda:0')

####################################################

# Example with K = 3 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432],
#           [-0.3521,  0.3413]]

# actions = [
#     [-0.0235, 0.1546],
#     [-0.0404, 0.0587],
#     [-0.0302, -0.0103],
#     [0.0678, -0.2968],
#     [-0.1852, -0.2063],
#     [-0.1246, -0.1617],
#     [-0.1639, -0.0897],
#     [-0.1362, 0.0396],
#     [-0.1967, 0.0291],
#     [-0.1149, 0.0416],
#     [-0.0877, -0.1905],
#     [-0.1191, -0.1912],
#     [-0.0177, -0.0839],
#     [-0.0824, -0.1845],
#     [-0.0198, 0.1253],
#     [-0.0464, -0.1017],
#     [0.0797, -0.0581],
#     [0.0329, -0.0225],
#     [-0.0327, -0.1217],
#     [-0.0030, 0.0578],
#     [0.1312, -0.0452],
#     [0.0678, -0.0360],
#     [-0.0237, -0.1756],
#     [0.0717, -0.1282],
#     [0.0162, -0.1396],
#     [0.0405, 0.0018],
#     [0.0749, -0.2586],
#     [0.1515, -0.2919],
#     [-0.0223, 0.0174],
#     [0.0855, -0.2002]
# ]

# Reward in sPCE
#torch.tensor([[-0.1659, -0.3293, -2.7324, -2.3580, -1.2444,  0.3015,  1.0161,  3.5981,
#          3.9865,  5.6176,  7.1921,  7.2251,  6.4561,  6.9053,  7.0904,  8.5603,
#          8.6577,  8.4892,  9.2406,  9.5968,  9.9950,  9.9933, 10.5498, 10.5222,
#         10.7455, 10.8643, 10.7432, 10.7087, 11.0801, 11.0319]],
#       device='cuda:0')

####################################################

# Example with K = 3 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402],
#           [-2.1387, -0.6234]]

# actions = [
#     [-0.0503, 0.0524],
#     [0.1885, -0.1468],
#     [0.2126, 0.2677],
#     [-0.2550, -0.1805],
#     [-0.0213, -0.4172],
#     [-0.3186, 0.0738],
#     [-0.0902, 0.2857],
#     [-0.0802, -0.0240],
#     [0.4228, -0.0017],
#     [0.0302, -0.0250],
#     [-0.1264, -0.0751],
#     [0.4447, 0.1305],
#     [-0.4961, -0.2627],
#     [-0.3824, -0.2161],
#     [-0.4023, -0.1316],
#     [-0.4372, -0.1845],
#     [-0.3679, -0.2371],
#     [-0.3597, -0.1971],
#     [-0.3483, -0.1713],
#     [-0.3841, -0.0269],
#     [-0.3799, -0.2933],
#     [-0.3198, -0.4127],
#     [-0.4633, -0.1771],
#     [-0.4528, -0.0116],
#     [-0.4262, 0.0154],
#     [-0.5192, -0.0602],
#     [-0.4563, -0.2456],
#     [-0.4241, 0.1031],
#     [-0.4342, -0.0506],
#     [-0.4051, -0.0356]
# ]

# Reward in sPCE
#torch.tensor([[ 5.9012,  6.8582,  7.7520,  8.3457,  9.0159,  9.7074, 10.2163,  9.8652,
#          9.6684,  9.5131, 10.0347, 10.3538, 12.4580, 12.8334, 13.3769, 13.5259,
#         13.5351, 13.5491, 13.5954, 13.6524, 13.6704, 13.6740, 13.4889, 13.3955,
#         13.4083, 13.7518, 13.7321, 13.7338, 13.7715, 13.7634]],
#       device='cuda:0')

####################################################

# Example with K = 3 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784],
#           [-2.6479, -0.0233]]

# actions = [
#     [-0.0062, 0.1560],
#     [0.2264, -0.1727],
#     [0.2857, 0.0209],
#     [-0.0396, -0.2549],
#     [-0.1138, -0.1864],
#     [-0.0541, -0.1584],
#     [-0.2485, -0.0950],
#     [0.0746, -0.3622],
#     [-0.1326, -0.0400],
#     [-0.2922, -0.1705],
#     [-0.2885, 0.1500],
#     [0.3534, 0.3574],
#     [-0.2229, -0.3980],
#     [-0.2332, -0.3501],
#     [-0.2082, -0.3648],
#     [-0.1619, -0.3455],
#     [-0.1865, -0.2971],
#     [-0.3177, -0.3613],
#     [-0.3528, -0.3158],
#     [-0.2476, -0.3043],
#     [-0.1131, -0.3069],
#     [-0.3569, -0.3813],
#     [-0.1215, -0.3874],
#     [-0.0788, 0.0539],
#     [-0.2972, -0.3019],
#     [-0.2566, -0.2564],
#     [-0.3680, -0.4542],
#     [-0.2037, -0.5823],
#     [-0.4015, -0.2201],
#     [-0.2744, -0.4628]
# ]

# Reward in sPCE
#torch.tensor([[ 2.4215,  3.2396,  3.0471,  3.8999,  4.0494,  4.1883,  5.1653,  5.4307,
#          4.0704,  4.6161,  5.1049,  5.0192,  6.0866,  6.4996,  6.9383,  6.9163,
#          9.1224,  9.2983,  9.3948,  9.8150,  9.7313,  9.7647,  9.7863,  9.8414,
#          9.8758,  9.9665,  9.7224,  9.8456, 10.5593, 10.5853]],
#       device='cuda:0')

####################################################

# Example with K = 4 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784],
#           [-2.6479, -0.0233],
#           [-1.3761,  1.1551]]

# actions = [
#     [0.0744, 0.0132],
#     [-0.1870, -0.1560],
#     [-0.2679, 0.0473],
#     [-0.1311, -0.2534],
#     [-0.2961, 0.1192],
#     [-0.1467, -0.1755],
#     [-0.0312, -0.3100],
#     [-0.2588, -0.1407],
#     [-0.0995, -0.2519],
#     [-0.0243, -0.2031],
#     [-0.2218, 0.2472],
#     [-0.1802, 0.3929],
#     [-0.0664, 0.4049],
#     [-0.3404, 0.0638],
#     [-0.0982, -0.0115],
#     [-0.4771, -0.0817],
#     [-0.3832, -0.2420],
#     [-0.4174, -0.1635],
#     [-0.3058, -0.3024],
#     [-0.4021, -0.2724],
#     [-0.2662, -0.3586],
#     [-0.2597, -0.4024],
#     [-0.1538, -0.4768],
#     [-0.2583, -0.3346],
#     [-0.3860, -0.2674],
#     [-0.2379, -0.4578],
#     [-0.2555, -0.1611],
#     [-0.4326, -0.1603],
#     [0.0109, -0.2781],
#     [-0.5578, -0.3298]
# ]

# Reward in sPCE
#torch.tensor([[ 3.2631,  4.1612,  4.9791,  6.2169,  5.6169,  5.7054,  5.6458,  6.0162,
#          6.1830,  6.4225,  6.5978,  7.2768,  7.6524,  7.8829,  7.8500,  8.5696,
#          9.1364,  9.2439,  9.6300,  9.7319, 10.3487, 10.5691, 10.8203, 10.9875,
#         11.1288, 11.1485, 11.2792, 11.0573, 10.8162, 11.0136]],
#       device='cuda:0')

####################################################

# Example with K = 4 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402],
#           [-2.1387, -0.6234],
#           [-0.4541, -0.6023]]

# actions = [
#     [0.0318, -0.1263],
#     [-0.0159, -0.0242],
#     [0.0842, 0.0635],
#     [-0.1329, -0.0700],
#     [-0.0684, -0.1186],
#     [-0.0911, -0.0398],
#     [0.0003, 0.2051],
#     [0.1658, 0.1650],
#     [-0.0972, 0.0592],
#     [0.2001, -0.1376],
#     [-0.0864, 0.0103],
#     [0.4110, -0.1070],
#     [-0.0964, -0.1841],
#     [0.0131, -0.1677],
#     [-0.0053, -0.0982],
#     [-0.0210, 0.0210],
#     [-0.0499, -0.0684],
#     [-0.0470, -0.1384],
#     [-0.0619, 0.0711],
#     [-0.0617, -0.2357],
#     [0.1242, -0.2080],
#     [0.0509, -0.2116],
#     [-0.0456, -0.2344],
#     [-0.0632, -0.3121],
#     [-0.0815, -0.2319],
#     [-0.1283, -0.1265],
#     [0.0484, 0.0056],
#     [-0.0886, 0.0758],
#     [-0.0626, -0.0427],
#     [0.0005, -0.1554]
# ]

# Reward in sPCE
#torch.tensor([[ 0.6288,  0.7059,  1.2918,  2.5632,  3.4374,  3.8445,  4.4818,  4.8492,
#          6.1222,  6.5021,  7.2075,  7.9796,  8.7689,  8.2986,  8.1180,  9.0828,
#          9.0883,  9.3541, 10.6134, 11.8117, 11.7130, 12.2302, 12.5910, 12.7827,
#         13.1988, 13.4250, 13.5282, 13.5091, 13.5071, 13.5075]],
#       device='cuda:0')

####################################################

# Example with K = 4 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432],
#           [-0.3521,  0.3413],
#           [-1.6456,  1.5698]]

# actions = [
#     [-0.0119, 0.1413],
#     [0.0773, -0.0123],
#     [-0.0496, -0.0193],
#     [-0.1162, -0.0634],
#     [-0.0479, -0.1120],
#     [0.0488, 0.2358],
#     [0.0979, 0.1344],
#     [0.0805, 0.1373],
#     [-0.0202, 0.1409],
#     [-0.0956, 0.1231],
#     [-0.0579, -0.0233],
#     [-0.0351, 0.0266],
#     [-0.0242, 0.0523],
#     [-0.0257, 0.0400],
#     [-0.0759, 0.0764],
#     [0.0501, -0.0171],
#     [-0.0261, 0.0740],
#     [-0.1623, 0.1168],
#     [-0.0846, 0.0419],
#     [-0.1476, 0.0523],
#     [-0.0838, 0.0262],
#     [-0.2041, 0.0253],
#     [-0.0805, 0.0093],
#     [-0.0306, 0.0794],
#     [-0.1192, 0.1469],
#     [-0.1130, 0.1629],
#     [-0.1028, -0.0617],
#     [-0.2920, 0.1474],
#     [-0.1999, -0.1043],
#     [0.0066, 0.2358]
# ]

# Reward in sPCE
#torch.tensor([[1.1206, 1.3154, 1.9660, 3.0857, 3.8492, 3.7683, 3.4377, 4.3689, 3.8896,
#         5.1549, 5.2868, 5.1395, 5.4441, 5.3877, 5.9343, 6.6978, 5.5567, 5.8876,
#         5.6594, 6.0411, 6.1377, 6.5671, 6.5955, 6.2575, 6.7964, 6.9354, 7.3427,
#         7.6699, 8.2752, 8.6022]], device='cuda:0')

####################################################

# Example with K = 5 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432],
#           [-0.3521,  0.3413],
#           [-1.6456,  1.5698],
#           [-0.2490, -0.4059]]

# actions = [
#     [-0.0058, 0.1396],
#     [0.0487, -0.0069],
#     [0.0821, 0.1699],
#     [0.0162, 0.0993],
#     [0.0912, -0.2684],
#     [0.1124, -0.2752],
#     [0.0816, -0.1969],
#     [0.1383, -0.1515],
#     [0.0755, -0.1393],
#     [0.1335, -0.0630],
#     [0.0796, 0.0535],
#     [0.0575, 0.0555],
#     [0.1701, -0.0165],
#     [0.0193, 0.1880],
#     [-0.0589, -0.1830],
#     [-0.0224, 0.1377],
#     [-0.0419, 0.1686],
#     [-0.0257, -0.1657],
#     [-0.0297, 0.1608],
#     [0.0085, 0.1714],
#     [0.0502, -0.1633],
#     [-0.0085, 0.2259],
#     [-0.0219, 0.0793],
#     [-0.0267, -0.1286],
#     [-0.1123, 0.2403],
#     [-0.1029, 0.1512],
#     [-0.0206, 0.2755],
#     [-0.1756, 0.2719],
#     [-0.1042, 0.2198],
#     [-0.0913, 0.2753]
# ]

# Reward in sPCE
#torch.tensor([[ 0.7403,  0.8159,  0.6000,  0.5370,  0.8826,  0.9509,  1.3983,  1.8125,
#          1.4860,  0.5955,  0.7569,  1.2324,  3.0203,  2.1083,  3.5354,  4.6231,
#          4.4957,  5.3156,  5.2009,  5.5387,  5.7422,  7.0927,  7.5020,  8.9330,
#          9.0218,  9.9532, 10.2546, 10.6244, 10.2508, 10.1224]],
#       device='cuda:0')

####################################################

# Example with K = 5 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784],
#           [-2.6479, -0.0233],
#           [-1.3761,  1.1551],
#           [-1.0914, -0.3823]]

# actions = [
#     [0.0476, 0.0807],
#     [-0.0692, -0.2505],
#     [0.1892, -0.2601],
#     [-0.1044, -0.0717],
#     [-0.2440, -0.1083],
#     [0.3625, -0.0116],
#     [0.2081, -0.3602],
#     [0.0892, -0.3147],
#     [-0.0975, -0.3014],
#     [-0.1343, -0.3327],
#     [-0.0729, -0.3533],
#     [-0.1755, -0.2954],
#     [-0.1545, -0.2254],
#     [-0.1659, -0.2672],
#     [-0.1673, -0.1617],
#     [-0.1863, -0.0659],
#     [-0.2653, -0.0621],
#     [-0.0318, -0.1420],
#     [-0.0379, -0.1523],
#     [-0.0609, -0.2249],
#     [0.0329, -0.1866],
#     [-0.0366, -0.1804],
#     [-0.1211, -0.2472],
#     [-0.0430, -0.0982],
#     [-0.0036, -0.2853],
#     [0.0099, -0.2016],
#     [-0.2577, -0.4390],
#     [-0.0729, -0.1878],
#     [0.2192, -0.1288],
#     [-0.2679, -0.3284]
# ]

# Reward in sPCE
#torch.tensor([[ 2.1090,  2.0124,  2.6652,  2.9112,  5.0540,  5.8540,  7.1482,  7.2557,
#          7.7871,  7.6825,  7.9208, 11.5046, 11.8077, 12.4236, 13.1470, 13.2934,
#         13.6642, 13.6865, 13.6934, 13.6869, 13.6914, 13.6914, 13.6953, 13.7130,
#         13.6901, 13.7323, 13.7390, 13.7485, 13.7462, 13.7367]],
#       device='cuda:0')

####################################################

# Example with K = 5 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402],
#           [-2.1387, -0.6234],
#           [-0.4541, -0.6023],
#           [ 0.3330, -2.1362]]

# actions = [
#     [-0.0794, 0.0130],
#     [-0.0034, -0.0131],
#     [-0.0387, 0.0919],
#     [0.2901, -0.1187],
#     [0.1999, 0.1236],
#     [-0.0274, 0.0881],
#     [-0.0103, 0.1107],
#     [-0.0545, -0.2574],
#     [0.1063, -0.3075],
#     [-0.0824, -0.2318],
#     [-0.0319, -0.2209],
#     [-0.0462, -0.1779],
#     [-0.0496, -0.1425],
#     [-0.0327, -0.1200],
#     [-0.0205, -0.2162],
#     [-0.0637, -0.0801],
#     [-0.1225, -0.1690],
#     [-0.0315, -0.0641],
#     [-0.0983, -0.0914],
#     [-0.1426, -0.1455],
#     [-0.1691, -0.1330],
#     [-0.1442, -0.0874],
#     [-0.2054, -0.0335],
#     [0.0200, -0.0009],
#     [-0.1642, 0.0773],
#     [-0.1544, -0.0929],
#     [-0.1672, -0.1866],
#     [-0.2151, -0.0637],
#     [-0.2140, -0.2357],
#     [-0.0946, -0.0876]
# ]

# Reward in sPCE
#torch.tensor([[ 1.6664,  2.1957,  3.8007,  4.7406,  5.1129,  5.3905,  5.5368,  6.3042,
#          6.6746,  6.7894,  6.6026,  6.7747,  7.0635,  7.6125,  7.8260,  7.9364,
#          8.7106,  8.7571,  8.1272,  8.5246,  8.6863,  8.3653,  8.6331,  8.4016,
#          9.8342, 10.7507, 10.9272, 10.9963, 11.2371, 11.3279]],
#       device='cuda:0')

####################################################

print(len(actions))

# Multiply each number by 4
scaled_actions = [[4 * x, 4 * y] for x, y in actions]

# Extracting x and y coordinates
x, y = zip(*scaled_actions)

# Points theta1 and theta2
#theta1 = np.array([-2.2659e+00, -1.7917e-02])
#theta2 = np.array([ 1.6182e-01, -2.6031e+00])

#theta1 = np.array([ 1.8805e+00, -1.1253e+00])
#theta2 = np.array([ 1.8882e-01, -7.5086e-01])

# Constants for the signal intensity equation
b = 1e-1
alpha1 = 1.0
alpha2 = 1.0
m = 1e-4

# Create a grid of points in the region around theta1 and theta2
grid_size = 100
xi = np.linspace(min(x) - 5, max(x) + 5, grid_size)
yi = np.linspace(min(y) - 5, max(y) + 5, grid_size)
xi, yi = np.meshgrid(xi, yi)

# Calculate the signal intensity at each point in the grid
#intensity = b + (alpha1 / (m + (np.square(xi - theta_single_k1[0] + np.square(yi - theta_single_k1[1]))) 
              #+ (alpha2 / (m + (np.square(xi - theta2[0] + np.square(yi - theta2[1])))

#cmap, norm = mcolors.from_levels_and_colors[0, 2, 5, 6], ['red', 'green', 'blue']

# Plot setup
fig, ax = plt.subplots(figsize=(8, 4.8))
scatter = ax.scatter(x, y, c=np.linspace(1, 30, len(scaled_actions)), cmap='viridis', label='Designs')
ax.scatter([i for i, j in thetas], [j for i, j in thetas], color='blue', label='Objects', marker='x')
ax.set_xlim((-4, 4))
ax.set_ylim((-4, 4))
ax.set_title(f"2D Location Finding with {len(thetas)} Objects")
ax.legend()

# Add colorbar below the plot
cbar = plt.colorbar(scatter, ax=ax, orientation='horizontal', fraction=0.03, pad=0.08)
cbar.set_label('Experiment Order')

plt.xlim((-4, 4))
plt.ylim((-4, 4))

# Adding titles and labels
plt.title(f"2D Location Finding with {len(thetas)} Objects")
#plt.xlabel('X-axis')
#plt.ylabel('Y-axis')
plt.legend()
plt.savefig(f"Location-Finding-Plots/location_finding_droq_0.01_k_{len(thetas)}.pdf", transparent=True, bbox_inches="tight")
#plt.show()