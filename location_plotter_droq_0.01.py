import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import torch

# Seeds are 10, 50, 100 for evaluation time in select_policy_env.py

# Example with K = 2 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432]]

# actions = [
#     [-0.0685, 0.0535],
#     [0.1438, 0.1110],
#     [0.0071, -0.1892],
#     [0.1159, -0.1618],
#     [-0.0641, -0.1945],
#     [-0.1121, -0.1731],
#     [-0.1353, -0.1562],
#     [-0.1255, -0.1227],
#     [-0.2310, 0.1168],
#     [0.0922, 0.4867],
#     [-0.0009, 0.3228],
#     [-0.0175, 0.2894],
#     [0.1421, 0.2914],
#     [0.0105, 0.3426],
#     [-0.0308, 0.3600],
#     [-0.1020, -0.0481],
#     [-0.0296, 0.3230],
#     [-0.0640, 0.2268],
#     [-0.0586, -0.1031],
#     [0.0195, 0.2644],
#     [-0.0331, -0.1393],
#     [0.0177, 0.3997],
#     [0.1398, 0.3296],
#     [0.0651, 0.3581],
#     [-0.0567, 0.1989],
#     [0.0507, -0.0537],
#     [-0.0081, -0.0360],
#     [0.0313, 0.1902],
#     [-0.0188, 0.0028],
#     [0.0352, 0.2335]
# ]

# Reward in sPCE
#torch.tensor([[ 0.3598,  0.9986,  2.4417,  3.5967,  4.5815,  5.5261,  5.9300,  6.6609,
#          6.8330,  8.8042,  9.6506, 10.6454, 10.8593, 10.2326, 10.4725,  9.9973,
#         10.0598, 10.0510, 10.4126,  9.0457,  9.8482, 10.4821, 12.9321, 13.0663,
#         12.9943, 12.9033, 13.0541, 13.0480, 13.0956, 13.0850]],
#       device='cuda:0')

####################################################

# Example with K = 2 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784]]

# actions = [
#     [-0.0232, 0.0282],
#     [0.1153, -0.2689],
#     [0.2073, -0.0711],
#     [-0.0547, -0.2144],
#     [-0.2715, 0.0855],
#     [0.1167, 0.2795],
#     [-0.1055, -0.3306],
#     [-0.1693, -0.3043],
#     [-0.1204, -0.3077],
#     [-0.0915, -0.3224],
#     [-0.3712, -0.1971],
#     [-0.1882, -0.3718],
#     [0.0072, -0.4342],
#     [-0.4463, -0.2314],
#     [-0.4200, -0.1656],
#     [-0.1915, -0.3135],
#     [-0.5656, 0.2033],
#     [-0.2207, -0.4360],
#     [-0.2323, -0.4197],
#     [-0.2892, -0.4773],
#     [0.1262, -0.3256],
#     [0.6464, -0.1629],
#     [-0.4187, -0.1598],
#     [0.0439, -0.5705],
#     [-0.0007, -0.5729],
#     [-0.4769, -0.0023],
#     [-0.4968, -0.0754],
#     [0.1912, -0.1552],
#     [-0.3383, -0.3917],
#     [-0.2905, -0.5765]
# ]

# Reward in sPCE
#torch.tensor([[0.2121, 0.4736, 1.1129, 2.2731, 2.8930, 4.0541, 5.0850, 6.0397, 6.2205,
#         6.4508, 6.5666, 6.6376, 7.2863, 7.2461, 7.2550, 8.6103, 8.6399, 8.4818,
#         8.4135, 8.2594, 8.7969, 8.9832, 8.9853, 9.0718, 9.0303, 8.9992, 8.9861,
#         9.8591, 9.8784, 9.7615]], device='cuda:0')

####################################################

# Example with K = 2 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402]]

# actions = [
#     [0.0433, -0.0125],
#     [-0.0346, 0.0247],
#     [-0.0477, 0.0515],
#     [0.3092, 0.0116],
#     [-0.1842, -0.2379],
#     [-0.1644, 0.4139],
#     [-0.0683, 0.4203],
#     [-0.3434, 0.2678],
#     [-0.3605, 0.2529],
#     [0.0212, 0.4743],
#     [0.0517, 0.2998],
#     [-0.2743, 0.2586],
#     [-0.0348, 0.1669],
#     [-0.0696, 0.1763],
#     [-0.3362, 0.1955],
#     [-0.1873, 0.1181],
#     [-0.0982, 0.1464],
#     [-0.1847, 0.0831],
#     [-0.0508, -0.0270],
#     [-0.2662, 0.0838],
#     [-0.1644, 0.0394],
#     [-0.2301, 0.1025],
#     [-0.2976, 0.1866],
#     [-0.1176, 0.1949],
#     [-0.0465, -0.1650],
#     [-0.0670, 0.2988],
#     [-0.2217, 0.0102],
#     [0.0672, 0.1310],
#     [-0.0293, 0.1408],
#     [-0.1035, 0.0429]
# ]

# Reward in sPCE
#torch.tensor([[ 0.8457,  3.0624,  5.3369,  6.4029,  7.5625,  8.4447,  8.8962,  9.8356,
#         10.1782, 10.2517, 10.4633, 10.6473, 10.7785, 10.8348, 11.0394, 11.0391,
#         11.3155, 11.5851, 11.6619, 11.7748, 11.9489, 11.9162, 11.8951, 11.8554,
#         11.8576, 11.8290, 11.8824, 11.9249, 11.7565, 12.0538]],
#       device='cuda:0')

####################################################
continue here
# Example with K = 1 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982]]

# actions = [
#     [0.0296, 0.0528],
#     [-0.3433, -0.1627],
#     [0.3237, -0.2966],
#     [0.3062, -0.1650],
#     [0.1895, -0.3341],
#     [-0.1500, 0.5426],
#     [0.4826, -0.0166],
#     [0.2811, -0.5066],
#     [0.1730, -0.4956],
#     [-0.3718, -0.5837],
#     [0.4558, 0.4807],
#     [0.5733, -0.4050],
#     [0.5221, -0.3130],
#     [0.5536, -0.3466],
#     [0.4412, -0.2842],
#     [0.3956, -0.2577],
#     [0.2670, -0.4164],
#     [0.5947, -0.1035],
#     [0.1613, -0.4492],
#     [-0.6330, 0.2246],
#     [0.5462, -0.1646],
#     [0.3414, -0.4786],
#     [0.0332, -0.5875],
#     [-0.2966, -0.5254],
#     [0.5372, 0.5308],
#     [0.2993, -0.4945],
#     [0.2842, -0.6631],
#     [0.6259, -0.4424],
#     [0.3487, -0.4076],
#     [0.4405, -0.4999]
# ]

# Reward in sPCE
#torch.tensor([[1.0147, 1.6593, 4.2382, 4.5429, 5.2652, 5.2772, 5.1009, 6.4621, 6.5847,
#         6.6030, 6.6258, 6.6563, 6.6825, 6.6249, 6.6715, 6.8128, 7.3231, 7.3270,
#         7.4166, 7.4372, 7.4840, 7.5487, 7.5553, 7.5665, 7.6106, 7.8803, 7.8878,
#         8.0160, 8.5653, 8.6478]], device='cuda:0')

####################################################

# Example with K = 1 and DroQ-0.01
# thetas = [[-1.4033,  1.8134]]

# actions = [
#     [0.0460, 0.0680],
#     [-0.3409, -0.2455],
#     [0.2482, -0.4167],
#     [-0.2587, 0.2618],
#     [-0.0742, 0.4571],
#     [0.6099, 0.0692],
#     [-0.4672, 0.1392],
#     [-0.0167, -0.7524],
#     [0.0459, 0.6682],
#     [-0.4966, 0.4411],
#     [-0.4755, 0.4787],
#     [-0.4779, 0.4426],
#     [-0.4037, 0.4197],
#     [-0.3813, 0.5479],
#     [-0.2576, 0.2904],
#     [-0.4496, 0.4802],
#     [-0.2941, 0.3930],
#     [-0.1964, 0.4887],
#     [-0.3805, 0.6063],
#     [-0.2613, 0.6836],
#     [0.0192, 0.2661],
#     [-0.3571, 0.5425],
#     [-0.1569, -0.0811],
#     [-0.4138, 0.4525],
#     [-0.2782, 0.5015],
#     [-0.2826, 0.6978],
#     [-0.0710, 0.6003],
#     [-0.2100, 0.0935],
#     [-0.1962, 0.1887],
#     [-0.0567, 0.4389]
# ]

# Reward in sPCE
#torch.tensor([[1.2309, 1.7981, 1.9029, 3.5449, 3.5083, 3.5212, 5.3354, 5.3493, 5.5543,
#         5.7131, 6.2796, 6.6607, 7.2650, 7.6398, 7.9593, 8.2658, 8.2192, 8.2817,
#         8.4073, 8.0876, 8.0452, 7.8483, 7.8724, 7.5188, 7.1060, 7.0899, 7.1466,
#         7.2987, 7.3857, 7.3674]], device='cuda:0')

####################################################

# Example with K = 1 and DroQ-0.01
# thetas = [[-0.1029,  1.6810]]

# actions = [
#     [0.0083, -0.0556],
#     [0.0911, 0.3127],
#     [-0.2683, 0.2287],
#     [0.4318, 0.0481],
#     [-0.2944, -0.2792],
#     [-0.2424, 0.5124],
#     [-0.0245, 0.5135],
#     [0.2023, -0.0754],
#     [0.2114, 0.5821],
#     [-0.5604, -0.0783],
#     [-0.2275, 0.5766],
#     [-0.1318, 0.5049],
#     [-0.1225, 0.4315],
#     [-0.2263, 0.4366],
#     [-0.0307, 0.4837],
#     [0.1133, 0.4939],
#     [0.0065, 0.4164],
#     [0.1968, 0.4181],
#     [0.1069, 0.4238],
#     [0.0349, 0.4240],
#     [0.1017, 0.3637],
#     [-0.0266, 0.5901],
#     [0.0260, 0.3871],
#     [0.0060, 0.5201],
#     [-0.0437, 0.4119],
#     [0.0831, 0.4619],
#     [0.0831, 0.2817],
#     [-0.1277, 0.7244],
#     [-0.0265, 0.5944],
#     [0.0210, 0.7022]
# ]

# Reward in sPCE
#torch.tensor([[0.6403, 0.3383, 1.0874, 2.0407, 2.1659, 2.3160, 3.3657, 3.9133, 5.0994,
#         5.1580, 4.9046, 5.3208, 5.3276, 5.5460, 5.7333, 6.1910, 6.5231, 6.7299,
#         7.2165, 7.1318, 7.2217, 7.3113, 7.7801, 7.9853, 8.6652, 8.5629, 8.5873,
#         8.5791, 8.5245, 8.3652]], device='cuda:0')

####################################################

# Example with K = 3 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432],
#           [-0.3521,  0.3413]]

# actions = [
#     [0.2649, -0.1097],
#     [-0.0384, 0.0205],
#     [-0.0090, -0.1947],
#     [-0.0118, -0.1792],
#     [-0.0443, -0.1257],
#     [0.1112, 0.2551],
#     [0.1629, 0.1309],
#     [0.0440, 0.2235],
#     [0.0993, 0.1632],
#     [0.0296, 0.2223],
#     [0.1742, 0.1578],
#     [-0.0239, 0.0916],
#     [0.0280, -0.1133],
#     [0.2111, 0.1914],
#     [0.1772, 0.1622],
#     [-0.0259, -0.0948],
#     [0.0144, 0.3226],
#     [-0.0110, 0.3264],
#     [0.0067, 0.3207],
#     [-0.0223, 0.3905],
#     [-0.0884, -0.1356],
#     [-0.0192, -0.3077],
#     [0.0004, 0.2413],
#     [-0.0060, 0.1841],
#     [-0.0288, 0.0322],
#     [0.0239, -0.1562],
#     [-0.0642, 0.2509],
#     [-0.0250, -0.1453],
#     [0.0347, -0.0187],
#     [0.1017, -0.2485]
# ]

# Reward in sPCE
#torch.tensor([[ 0.6756, -2.0092, -0.4510,  0.0328,  1.6702,  2.6430,  2.3067,  2.7754,
#          3.1183,  3.5022,  1.9952,  2.8149,  3.8176,  5.5452,  5.8043,  5.8833,
#          7.0521,  7.0250,  7.7985, 11.3502, 12.3402, 12.3333, 12.2653, 12.4635,
#         13.5992, 13.6124, 13.7792, 13.7816, 13.7915, 13.7919]],
#       device='cuda:0')

####################################################

# Example with K = 3 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402],
#           [-2.1387, -0.6234]]

# actions = [
#     [0.0263, 0.0498],
#     [0.0617, -0.1720],
#     [-0.0928, 0.0747],
#     [-0.1224, 0.0526],
#     [0.2322, -0.0878],
#     [-0.0787, 0.0618],
#     [0.1966, 0.3322],
#     [-0.3530, -0.1297],
#     [-0.3366, -0.0911],
#     [-0.2587, -0.1750],
#     [-0.3107, -0.0785],
#     [-0.1056, 0.1030],
#     [-0.1658, 0.0925],
#     [-0.1673, -0.2186],
#     [-0.1955, 0.1042],
#     [-0.3523, -0.3281],
#     [-0.2946, -0.2319],
#     [-0.3716, -0.3736],
#     [-0.1996, -0.2693],
#     [-0.4093, 0.0381],
#     [-0.4613, 0.0391],
#     [-0.2953, 0.0712],
#     [-0.4517, -0.1034],
#     [-0.3798, -0.1004],
#     [-0.3169, 0.0178],
#     [-0.0476, -0.0402],
#     [-0.2812, 0.0043],
#     [-0.4598, -0.1174],
#     [-0.4636, -0.1155],
#     [-0.5450, -0.1164]
# ]

# Reward in sPCE
#torch.tensor([[-1.4155, -0.3330,  2.4697,  3.3547,  3.6640,  5.7304,  6.3215,  6.8375,
#          7.1482,  6.9630,  6.5342,  6.7893,  6.7001,  7.0746,  7.9748,  8.0213,
#          8.7256,  9.0556,  9.2590,  9.0754, 10.4279, 10.7282, 10.8830, 11.0387,
#         11.0135, 10.8414, 10.7736, 10.5191, 10.6467, 10.2668]],
#       device='cuda:0')

####################################################

# Example with K = 3 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784],
#           [-2.6479, -0.0233]]

# actions = [
#     [0.0327, 0.0768],
#     [-0.0176, -0.2994],
#     [-0.1912, -0.2092],
#     [-0.2196, -0.1626],
#     [-0.1634, -0.0989],
#     [0.1119, -0.1010],
#     [0.0930, -0.2623],
#     [0.3348, -0.1892],
#     [0.2713, -0.1144],
#     [0.2360, -0.2685],
#     [0.0061, -0.3077],
#     [-0.1996, -0.0958],
#     [0.4388, -0.0296],
#     [0.2798, -0.3207],
#     [-0.0647, -0.3496],
#     [0.2267, -0.2466],
#     [-0.1402, -0.0547],
#     [-0.0367, -0.3709],
#     [0.2290, -0.3895],
#     [-0.1777, -0.1397],
#     [-0.1592, -0.3354],
#     [-0.0809, -0.3241],
#     [0.2572, -0.2696],
#     [0.3954, -0.2926],
#     [0.3340, -0.2711],
#     [-0.1225, -0.2582],
#     [-0.1659, -0.2480],
#     [-0.1398, -0.3125],
#     [0.1991, -0.2533],
#     [0.2981, -0.1783]
# ]

# Reward in sPCE
#torch.tensor([[ 1.9855,  2.3518,  2.4320,  2.8533,  3.6708,  2.0347,  1.8219,  2.1436,
#          3.8277,  4.6787,  6.6096,  6.9058,  7.4431,  7.8596,  8.2940,  8.1917,
#          8.4775,  8.0446,  8.4639,  8.5131,  9.2400,  9.3411,  9.2021, 10.8325,
#         11.0371, 12.1815, 12.6124, 12.4206, 12.6436, 12.6919]],
#       device='cuda:0')

####################################################

# Example with K = 4 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784],
#           [-2.6479, -0.0233],
#           [-1.3761,  1.1551]]

# actions = [
#     [0.0443, 0.0397],
#     [0.0296, -0.2657],
#     [0.1990, -0.1494],
#     [-0.1024, -0.1234],
#     [0.0207, 0.2246],
#     [-0.2307, 0.0637],
#     [0.3411, 0.2688],
#     [-0.2359, -0.1528],
#     [-0.2916, -0.2052],
#     [-0.3704, 0.0636],
#     [-0.1015, -0.2776],
#     [-0.1784, 0.2144],
#     [-0.2113, -0.2322],
#     [-0.2073, -0.2452],
#     [-0.1581, 0.6057],
#     [-0.2507, 0.5229],
#     [-0.3841, 0.3765],
#     [-0.2492, 0.3478],
#     [-0.2511, 0.3710],
#     [-0.2381, -0.3010],
#     [-0.2415, 0.3667],
#     [-0.2400, -0.3321],
#     [-0.2235, 0.3543],
#     [-0.2204, 0.3578],
#     [-0.2523, 0.3369],
#     [-0.2898, -0.2170],
#     [-0.2917, -0.4053],
#     [-0.1938, 0.0629],
#     [-0.1327, 0.3834],
#     [-0.2648, 0.2511]
# ]

# Reward in sPCE
#torch.tensor([[ 0.0295,  0.8235,  1.2844,  1.8853,  3.9045,  4.8466,  6.1318,  6.6081,
#          7.1490,  7.4116,  8.3127,  7.8280,  6.9858,  8.0862,  8.7909,  9.3208,
#         10.0159,  7.6152, 11.0574, 12.3254, 13.2027, 13.3080, 13.3903, 13.4326,
#         13.4057, 13.3983, 13.6381, 13.5840, 13.5675, 13.7032]],
#       device='cuda:0')

####################################################

# Example with K = 4 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402],
#           [-2.1387, -0.6234],
#           [-0.4541, -0.6023]]

# actions = [
#     [0.0382, 0.0036],
#     [-0.0537, -0.0968],
#     [-0.0801, -0.0565],
#     [-0.0559, -0.0151],
#     [-0.1071, -0.0468],
#     [-0.0145, 0.1030],
#     [-0.0687, 0.1077],
#     [-0.0710, 0.1509],
#     [-0.0441, 0.0732],
#     [-0.1126, 0.0631],
#     [-0.1826, 0.0274],
#     [-0.0334, -0.1511],
#     [-0.0076, -0.1115],
#     [-0.0233, -0.1616],
#     [-0.1446, -0.1275],
#     [-0.0318, -0.1886],
#     [-0.1305, -0.0910],
#     [-0.0225, -0.0637],
#     [-0.0141, -0.0452],
#     [0.0843, -0.0934],
#     [-0.1209, -0.1183],
#     [-0.0277, 0.0604],
#     [-0.0187, 0.0185],
#     [0.0151, -0.1640],
#     [-0.0813, -0.0071],
#     [0.0072, -0.0203],
#     [0.0357, -0.0600],
#     [-0.1786, -0.1832],
#     [-0.1314, -0.1291],
#     [-0.1875, -0.1249]
# ]

# Reward in sPCE
#torch.tensor([[ 0.4174,  1.8715,  2.1618,  2.7473,  3.6776,  5.1057,  5.6114,  6.1534,
#          9.1742,  9.4174,  9.5850, 10.3543, 10.5708, 10.8902, 11.4779, 11.7095,
#         11.2414, 11.0856, 11.0836, 11.0617, 11.7338, 12.1017, 12.1964, 12.3498,
#         12.3895, 12.4025, 12.4072, 12.8919, 13.1620, 13.1852]],
#       device='cuda:0')

####################################################

# Example with K = 4 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432],
#           [-0.3521,  0.3413],
#           [-1.6456,  1.5698]]

# actions = [
#     [0.0550, -0.0797],
#     [0.1107, 0.0210],
#     [-0.0771, -0.0367],
#     [-0.0140, -0.0909],
#     [0.0101, -0.1241],
#     [-0.0268, -0.0631],
#     [-0.0449, -0.0118],
#     [-0.0267, 0.2611],
#     [-0.0630, 0.1043],
#     [-0.0857, 0.1460],
#     [-0.0747, 0.0710],
#     [-0.0940, -0.0656],
#     [-0.0592, -0.1599],
#     [0.0512, -0.1921],
#     [-0.0532, -0.1663],
#     [0.0608, -0.0524],
#     [0.0062, 0.0841],
#     [-0.0814, 0.0475],
#     [-0.1310, 0.0089],
#     [0.0079, -0.3024],
#     [-0.1328, 0.0800],
#     [-0.0379, 0.0557],
#     [0.0129, 0.1518],
#     [0.0831, -0.0852],
#     [-0.1039, -0.2101],
#     [0.0043, -0.2102],
#     [0.0418, 0.0541],
#     [0.1225, 0.0133],
#     [-0.0944, -0.0936],
#     [-0.0348, -0.2008]
# ]

# Reward in sPCE
#torch.tensor([[ 0.0542,  1.2194,  2.1894,  2.4454,  3.2267,  2.0319,  2.5711,  2.7742,
#          6.2341,  6.6413,  7.6654,  7.2053,  8.1338,  8.4185,  9.3089,  8.8203,
#          8.8146,  9.1061,  9.3952, 10.2513, 10.5190, 11.5081, 11.4426, 12.1138,
#         12.8351, 12.9777, 12.9941, 13.0582, 13.2392, 13.2731]],
#       device='cuda:0')

####################################################

# Example with K = 5 and DroQ-0.01
# thetas = [[-0.1029,  1.6810],
#           [-0.2708, -0.5432],
#           [-0.3521,  0.3413],
#           [-1.6456,  1.5698],
#           [-0.2490, -0.4059]]

# actions = [
#     [0.1312, -0.0757],
#     [0.0160, 0.0437],
#     [0.0261, 0.0117],
#     [-0.0158, -0.1124],
#     [0.0074, 0.1809],
#     [0.1012, 0.1342],
#     [-0.0602, 0.0850],
#     [0.0243, 0.0038],
#     [-0.0268, -0.0928],
#     [0.1061, -0.0658],
#     [0.0106, 0.0883],
#     [0.1503, -0.0690],
#     [0.0194, 0.0319],
#     [-0.0482, 0.0968],
#     [-0.0336, 0.1272],
#     [0.0405, -0.1343],
#     [-0.1355, 0.1409],
#     [-0.0502, 0.0088],
#     [-0.1285, 0.1865],
#     [0.0020, 0.1269],
#     [-0.0368, 0.2245],
#     [0.0604, -0.0174],
#     [-0.1061, 0.0974],
#     [-0.1135, 0.0232],
#     [0.0206, -0.1526],
#     [0.0290, -0.1301],
#     [-0.1312, -0.0031],
#     [-0.0192, -0.1210],
#     [-0.0002, -0.1003],
#     [-0.0956, -0.1732]
# ]

# Reward in sPCE
#torch.tensor([[ 0.5883,  0.9191,  1.5705,  2.9686,  3.6624,  4.2554,  6.4347,  6.7160,
#          7.6428,  7.3110,  7.7248,  7.8278,  7.7893,  7.8658,  7.9975,  8.5085,
#          8.5112,  8.6527,  8.6674,  8.7532,  7.8462,  7.9774,  8.5252,  8.5278,
#          8.7187,  8.9129,  9.0367,  9.9441, 10.2112, 12.2705]],
#       device='cuda:0')

####################################################

# Example with K = 5 and DroQ-0.01
# thetas = [[ 1.4350, -1.4982],
#           [-0.8099, -1.1784],
#           [-2.6479, -0.0233],
#           [-1.3761,  1.1551],
#           [-1.0914, -0.3823]]

# actions = [
#     [0.0405, 0.0571],
#     [-0.0694, -0.2670],
#     [-0.1791, -0.1680],
#     [-0.1596, -0.1125],
#     [-0.2582, -0.0757],
#     [0.2606, -0.1700],
#     [0.0794, -0.3307],
#     [0.1317, -0.3074],
#     [0.0731, -0.2525],
#     [0.0132, -0.1479],
#     [0.4137, -0.0846],
#     [-0.1680, -0.2408],
#     [-0.1836, -0.0935],
#     [-0.2323, -0.1805],
#     [-0.1770, -0.0815],
#     [-0.1991, -0.3066],
#     [-0.2076, 0.0768],
#     [-0.1131, -0.0553],
#     [-0.1965, 0.0398],
#     [-0.2132, -0.3153],
#     [-0.1110, 0.0637],
#     [0.0039, 0.0518],
#     [-0.2498, 0.0744],
#     [-0.1217, -0.2032],
#     [-0.0934, 0.1328],
#     [-0.2019, 0.1212],
#     [-0.1232, -0.1699],
#     [-0.1198, 0.0411],
#     [-0.0316, -0.0225],
#     [-0.0668, -0.3383]
# ]

# Reward in sPCE
#torch.tensor([[3.1015, 3.6326, 4.7147, 4.9051, 7.3167, 7.8756, 8.5171, 8.3845, 8.6003,
#         8.3896, 8.0428, 9.2654, 9.4357, 9.7232, 9.7610, 9.1050, 8.5242, 8.5715,
#         6.4152, 7.0962, 7.5576, 7.6486, 9.4425, 9.1010, 9.2031, 9.1410, 8.8063,
#         8.9203, 8.9278, 8.9760]], device='cuda:0')

####################################################

# Example with K = 5 and DroQ-0.01
# thetas = [[-1.4033,  1.8134],
#           [-0.2177,  0.2402],
#           [-2.1387, -0.6234],
#           [-0.4541, -0.6023],
#           [ 0.3330, -2.1362]]

# actions = [
#     [0.0221, 0.0397],
#     [-0.0057, 0.0035],
#     [-0.0109, 0.0267],
#     [-0.0047, 0.0464],
#     [-0.0350, 0.0133],
#     [-0.0848, 0.0529],
#     [-0.1093, 0.0094],
#     [-0.0951, 0.0409],
#     [0.2895, -0.0662],
#     [-0.1189, -0.0531],
#     [-0.1147, 0.0121],
#     [-0.0321, -0.2280],
#     [-0.0529, -0.1806],
#     [0.0325, -0.2699],
#     [-0.0612, -0.1666],
#     [-0.0291, -0.1410],
#     [-0.0937, -0.1408],
#     [0.0236, -0.1431],
#     [-0.0590, -0.0770],
#     [-0.0316, -0.0576],
#     [-0.0058, -0.0662],
#     [0.0646, -0.1440],
#     [-0.0347, -0.1169],
#     [0.0177, -0.0238],
#     [0.0419, -0.0558],
#     [-0.0280, -0.0499],
#     [0.0314, -0.0648],
#     [-0.0962, -0.1324],
#     [0.0231, 0.1328],
#     [-0.0377, 0.0547]
# ]

# Reward in sPCE
#torch.tensor([[ 0.6938,  1.2193,  0.9879,  1.3025,  1.4874,  2.8461,  3.3459,  3.5898,
#          5.1620,  5.6177,  6.6762,  7.5043,  7.6995,  8.2546,  7.8282,  7.4798,
#         10.7118, 10.7240, 10.8769, 10.8976, 10.9055, 11.0227, 11.1913, 11.2242,
#         11.2331, 11.1931, 11.2308, 11.6085, 12.0322, 13.2314]],
#       device='cuda:0')

####################################################

print(len(actions))

# Multiply each number by 4
scaled_actions = [[4 * x, 4 * y] for x, y in actions]

# Extracting x and y coordinates
x, y = zip(*scaled_actions)

# Points theta1 and theta2
#theta1 = np.array([-2.2659e+00, -1.7917e-02])
#theta2 = np.array([ 1.6182e-01, -2.6031e+00])

#theta1 = np.array([ 1.8805e+00, -1.1253e+00])
#theta2 = np.array([ 1.8882e-01, -7.5086e-01])

# Constants for the signal intensity equation
b = 1e-1
alpha1 = 1.0
alpha2 = 1.0
m = 1e-4

# Create a grid of points in the region around theta1 and theta2
grid_size = 100
xi = np.linspace(min(x) - 5, max(x) + 5, grid_size)
yi = np.linspace(min(y) - 5, max(y) + 5, grid_size)
xi, yi = np.meshgrid(xi, yi)

# Calculate the signal intensity at each point in the grid
#intensity = b + (alpha1 / (m + (np.square(xi - theta_single_k1[0] + np.square(yi - theta_single_k1[1]))) 
              #+ (alpha2 / (m + (np.square(xi - theta2[0] + np.square(yi - theta2[1])))

#cmap, norm = mcolors.from_levels_and_colors[0, 2, 5, 6], ['red', 'green', 'blue']

# Plot setup
fig, ax = plt.subplots(figsize=(8, 4.8))
scatter = ax.scatter(x, y, c=np.linspace(1, 30, len(scaled_actions)), cmap='viridis', label='Designs')
ax.scatter([i for i, j in thetas], [j for i, j in thetas], color='blue', label='Objects', marker='x')
ax.set_xlim((-4, 4))
ax.set_ylim((-4, 4))
ax.set_title(f"2D Location Finding with {len(thetas)} Objects")
ax.legend()

# Add colorbar below the plot
cbar = plt.colorbar(scatter, ax=ax, orientation='horizontal', fraction=0.03, pad=0.08)
cbar.set_label('Experiment Order')

plt.xlim((-4, 4))
plt.ylim((-4, 4))

# Adding titles and labels
plt.title(f"2D Location Finding with {len(thetas)} Objects")
#plt.xlabel('X-axis')
#plt.ylabel('Y-axis')
plt.legend()
plt.savefig(f"location_finding_droq_0.01_k_{len(thetas)}.pdf", transparent=True, bbox_inches="tight")
#plt.show()